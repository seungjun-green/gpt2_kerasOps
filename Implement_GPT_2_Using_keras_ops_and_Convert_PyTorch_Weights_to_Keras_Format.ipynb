{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dUeTvGqOTKnU",
        "olBk8oOtmUDb",
        "kMnKL19LA2wB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implement GPT-2 Using keras.ops and Convert PyTorch Weights to Keras Format"
      ],
      "metadata": {
        "id": "XazpWMwEz7h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "mYXk4C-m0J8N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "K8I0tl_DDVvd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import backend as K\n",
        "import keras.ops as ops\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Model, GPT2Tokenizer\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement GPT2 using Keras.ops"
      ],
      "metadata": {
        "id": "Nfi682DsE2xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Layer**"
      ],
      "metadata": {
        "id": "3_04a7naTE5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDense(keras.layers.Layer):\n",
        "  def __init__(self, units, activation=None, name=None, **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "    self.units = units\n",
        "    self.activation = keras.activations.get(activation)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_dim = input_shape[-1]\n",
        "\n",
        "    self.kernel = self.add_weight(\n",
        "        shape=(input_dim, self.units),\n",
        "        initializer=\"glorot_uniform\",\n",
        "        trainable=True,\n",
        "        name=\"kernel\"\n",
        "    )\n",
        "\n",
        "    self.bias = self.add_weight(\n",
        "        shape=(self.units,),\n",
        "        initializer=\"zeros\",\n",
        "        trainable=True,\n",
        "        name=\"bias\"\n",
        "    )\n",
        "\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, inputs):\n",
        "    outputs = ops.matmul(inputs, self.kernel)\n",
        "    outputs = ops.add(outputs, self.bias)\n",
        "\n",
        "    if self.activation is not None:\n",
        "      outputs = self.activation(outputs)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "_IGBViYfEi2e"
      },
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Layer**"
      ],
      "metadata": {
        "id": "dUeTvGqOTKnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomEmbedding(keras.layers.Layer):\n",
        "  def __init__(self, input_dim, output_dim, embeddings_initializer=\"uniform\", mask_zero=False, name=None, **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.embeddings_initializer = keras.initializers.get(embeddings_initializer)\n",
        "    self.mask_zero = mask_zero\n",
        "    self.supports_masking = self.mask_zero\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # here input_dim is vocab size and output dim is embedding dim\n",
        "    self.embeddings = self.add_weight(\n",
        "        shape=(self.input_dim, self.output_dim),\n",
        "        initializer=self.embeddings_initializer,\n",
        "        name=\"embeddings\"\n",
        "    )\n",
        "    self.built=True\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # embeddings has shape of (vocab_szie, embedding_dim).\n",
        "    # u can think of each row represent a certain word.\n",
        "    outputs = keras.ops.take(self.embeddings, inputs, axis=0)\n",
        "    return outputs\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"input_dim\": self.input_dim,\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"embeddings_initializer\": keras.initializers.serialize(self.embeddings_initializer),\n",
        "        \"mask_zero\": self.mask_zero,\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "smUvdTGHTLqB"
      },
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MHA**"
      ],
      "metadata": {
        "id": "olBk8oOtmUDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMHA(keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, weight_initializer=\"glorot_uniform\", causal=False, name=None, **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "    self.causal = causal\n",
        "    self.weight_initializer = keras.initializers.get(weight_initializer)\n",
        "\n",
        "\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.W_qkv = self.add_weight(shape=(self.d_model, 3*self.d_model), initializer=self.weight_initializer, name=\"W_qkv\")\n",
        "    self.b_qkv = self.add_weight(shape=(3*self.d_model,), initializer=\"zeros\", name=\"b_qkv\")\n",
        "    self.W_o = self.add_weight(shape=(self.d_model, self.d_model), initializer=self.weight_initializer, name=\"W_o\")\n",
        "    self.b_o = self.add_weight(shape=(self.d_model,), initializer=\"zeros\", name=\"b_o\")\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    qkv = ops.matmul(x, self.W_qkv) # (N, T, D) @ (D, 3D) => (N, T, 3D)\n",
        "    qkv = ops.add(qkv, self.b_qkv) # gptw had bais term (N, T, 3D)\n",
        "    q, k, v = ops.split(qkv, 3, axis=-1)\n",
        "    # q: (N, T, D)\n",
        "    # v: (N, T, D)\n",
        "    # v: (N, T, D)\n",
        "\n",
        "    new_shape = (ops.shape(q)[0], ops.shape(q)[1], self.num_heads, self.d_k)\n",
        "    q = ops.reshape(q, new_shape) # (N, T, num_heads, d_k)\n",
        "    k = ops.reshape(k, new_shape) # (N, T, num_heads, d_k)\n",
        "    v = ops.reshape(v, new_shape) # (N, T, num_heads, d_k)\n",
        "\n",
        "    q = ops.transpose(q, (0, 2, 1, 3)) # (N, num_heads, T, d_k)\n",
        "    k = ops.transpose(k, (0, 2, 1, 3)) # (N, num_heads, T, d_k)\n",
        "    v = ops.transpose(v, (0, 2, 1, 3)) # (N, num_heads, T, d_k)\n",
        "\n",
        "\n",
        "    dk_float = ops.cast(self.d_k, \"float32\")\n",
        "    scale = ops.sqrt(dk_float)\n",
        "\n",
        "\n",
        "    # (N, num_heads, T, d_k) @ (N, num_heads, d_k, T) => (N, num_heads, T, T)\n",
        "    logits = ops.matmul(q, ops.transpose(k, (0, 1, 3, 2)))\n",
        "    logits = logits / scale # (N, num_heads, T, T)\n",
        "\n",
        "\n",
        "    if self.causal:\n",
        "      seq_len = ops.shape(logits)[-1]\n",
        "      causal_mask = ops.tril(ops.ones((seq_len, seq_len)))\n",
        "      logits = logits + (1.0 - causal_mask) * -1e9\n",
        "      '''\n",
        "      the mask we're adding is:\n",
        "      [\n",
        "        0 -e9 -e9\n",
        "        0 0 -e9\n",
        "        0 0 0\n",
        "      ]\n",
        "      '''\n",
        "\n",
        "    # (N, num_heads, T, T)\n",
        "    weights = ops.softmax(logits, axis=-1)\n",
        "    attn_out = ops.matmul(weights, v)\n",
        "    # (N, num_heads, T, d_v)\n",
        "\n",
        "    attn_out = ops.transpose(attn_out, (0, 2, 1, 3))\n",
        "    out_shape = (ops.shape(attn_out)[0], ops.shape(attn_out)[1], self.num_heads * self.d_k)\n",
        "    attn_out = ops.reshape(attn_out, out_shape)\n",
        "    # (N, T, d_model)\n",
        "\n",
        "    attn_out = ops.matmul(attn_out, self.W_o)\n",
        "    # (N, T, d_model)\n",
        "    attn_out = ops.add(attn_out, self.b_o)\n",
        "    # (N, T, d_model)\n",
        "\n",
        "    return attn_out"
      ],
      "metadata": {
        "id": "BxKeIDah6gie"
      },
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FeedForwardNetwork**"
      ],
      "metadata": {
        "id": "kMnKL19LA2wB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(keras.layers.Layer):\n",
        "  def __init__(self, d_model, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.d_model = d_model\n",
        "\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # (N, T, d_model\n",
        "    self.ff1 = CustomDense(units=self.d_model * 4, activation=keras.activations.gelu)\n",
        "    # (N, T, 4*d_model)\n",
        "    self.ff2 = CustomDense(units=self.d_model, activation=None)\n",
        "    # (N, T, d_model)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    inputs = self.ff1(inputs)\n",
        "    inputs = self.ff2(inputs)\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "Bx7cZin2A5Kv"
      },
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder Layer**"
      ],
      "metadata": {
        "id": "XT1QE24zHNgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, weight_initializer=\"uniform\", causal=True, name=None, **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.causal = causal\n",
        "\n",
        "    self.self_attention = CustomMHA(d_model, num_heads, weight_initializer=weight_initializer, causal=causal)\n",
        "    self.feed_forward = FeedFoward(d_model)\n",
        "\n",
        "    self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.layernorm1(x)\n",
        "    attn = self.self_attention(x)\n",
        "    x = ops.add(x, attn)\n",
        "\n",
        "    x = self.layernorm2(x)\n",
        "    ff_output = self.feed_forward(x)\n",
        "    x = ops.add(x, ff_output)\n",
        "\n",
        "\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "7MtUb6HrGLh3"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT2**"
      ],
      "metadata": {
        "id": "hUqae1-tzjl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, vocab_size, max_pos_encoding=1024, name=None, **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.final_ln = keras.layers.LayerNormalization(epsilon=1e-6, name=\"ln_f\")\n",
        "    self.embedding = CustomEmbedding(input_dim=vocab_size, output_dim=d_model)\n",
        "    self.pos_embedding = self.add_weight(shape=(max_pos_encoding, self.d_model), initializer=\"uniform\", trainable=True, name=\"pos_embedding\")\n",
        "    self.dec_layers = [TransformerDecoderLayer(d_model, num_heads, causal=True, name=f\"decoder_layer_{i}\") for i in range(num_layers)]\n",
        "    self.dropout = keras.layers.Dropout(0.1)\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    seq_len = x.shape[1]\n",
        "    x = self.embedding(x)\n",
        "    pos_embeds = self.pos_embedding[:seq_len, :]\n",
        "    pos_embeds = ops.broadcast_to(pos_embeds, ops.shape(x))\n",
        "    x = ops.add(x, pos_embeds)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.dec_layers[i](x)\n",
        "\n",
        "    x = self.final_ln(x)\n",
        "    final_logits = ops.matmul(x, ops.transpose(self.embedding.embeddings, (1, 0)))\n",
        "\n",
        "    return final_logits"
      ],
      "metadata": {
        "id": "1KEgmB6YN4Td"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "d_model = 768\n",
        "num_heads = 12\n",
        "num_layers = 12\n",
        "seq_len = 512\n",
        "batch_size = 1\n",
        "\n",
        "# check whether the output of keras-gpt2 has correct shape.\n",
        "gpt2 = TransformerDecoder(num_layers, d_model, num_heads, vocab_size)\n",
        "sample_input = np.zeros((batch_size, seq_len), dtype='int32')\n",
        "sample_output = gpt2(sample_input)\n",
        "print(sample_output.shape) # (B, T, D)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR4EqU_XOP14",
        "outputId": "601b47fa-af81-487b-f531-23d81e3a6ec3"
      },
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 512, 50257)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Weight Keys"
      ],
      "metadata": {
        "id": "_hQH-_Cbhblb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for weight in gpt2.weights:\n",
        "  print(weight.name)\n",
        "  count += 1\n",
        "\n",
        "print(f\"{count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0uknXdDVIZQ2",
        "outputId": "853419e1-79a5-453a-9fea-fbddc1104a2b"
      },
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_embedding\n",
            "gamma\n",
            "beta\n",
            "embeddings\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "W_qkv\n",
            "b_qkv\n",
            "W_o\n",
            "b_o\n",
            "kernel\n",
            "bias\n",
            "kernel\n",
            "bias\n",
            "gamma\n",
            "beta\n",
            "gamma\n",
            "beta\n",
            "148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "hf_model = GPT2Model.from_pretrained(model_name)\n",
        "hf_model.train() # this is to ensure dropout is also enabled for hf gpt2\n",
        "hf_state_dict = hf_model.state_dict()\n",
        "numpy_state_dict = {k: v.cpu().numpy() for k, v in hf_state_dict.items()}"
      ],
      "metadata": {
        "id": "nfCv6Ax0EQr9"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for key in numpy_state_dict.keys():\n",
        "  count += 1\n",
        "  print(key)\n",
        "\n",
        "print(f\"total num of keys: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Tn2zeYQ3H7BA",
        "outputId": "f0992589-5f31-4aba-d3d3-55177cbe5420"
      },
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wte.weight\n",
            "wpe.weight\n",
            "h.0.ln_1.weight\n",
            "h.0.ln_1.bias\n",
            "h.0.attn.c_attn.weight\n",
            "h.0.attn.c_attn.bias\n",
            "h.0.attn.c_proj.weight\n",
            "h.0.attn.c_proj.bias\n",
            "h.0.ln_2.weight\n",
            "h.0.ln_2.bias\n",
            "h.0.mlp.c_fc.weight\n",
            "h.0.mlp.c_fc.bias\n",
            "h.0.mlp.c_proj.weight\n",
            "h.0.mlp.c_proj.bias\n",
            "h.1.ln_1.weight\n",
            "h.1.ln_1.bias\n",
            "h.1.attn.c_attn.weight\n",
            "h.1.attn.c_attn.bias\n",
            "h.1.attn.c_proj.weight\n",
            "h.1.attn.c_proj.bias\n",
            "h.1.ln_2.weight\n",
            "h.1.ln_2.bias\n",
            "h.1.mlp.c_fc.weight\n",
            "h.1.mlp.c_fc.bias\n",
            "h.1.mlp.c_proj.weight\n",
            "h.1.mlp.c_proj.bias\n",
            "h.2.ln_1.weight\n",
            "h.2.ln_1.bias\n",
            "h.2.attn.c_attn.weight\n",
            "h.2.attn.c_attn.bias\n",
            "h.2.attn.c_proj.weight\n",
            "h.2.attn.c_proj.bias\n",
            "h.2.ln_2.weight\n",
            "h.2.ln_2.bias\n",
            "h.2.mlp.c_fc.weight\n",
            "h.2.mlp.c_fc.bias\n",
            "h.2.mlp.c_proj.weight\n",
            "h.2.mlp.c_proj.bias\n",
            "h.3.ln_1.weight\n",
            "h.3.ln_1.bias\n",
            "h.3.attn.c_attn.weight\n",
            "h.3.attn.c_attn.bias\n",
            "h.3.attn.c_proj.weight\n",
            "h.3.attn.c_proj.bias\n",
            "h.3.ln_2.weight\n",
            "h.3.ln_2.bias\n",
            "h.3.mlp.c_fc.weight\n",
            "h.3.mlp.c_fc.bias\n",
            "h.3.mlp.c_proj.weight\n",
            "h.3.mlp.c_proj.bias\n",
            "h.4.ln_1.weight\n",
            "h.4.ln_1.bias\n",
            "h.4.attn.c_attn.weight\n",
            "h.4.attn.c_attn.bias\n",
            "h.4.attn.c_proj.weight\n",
            "h.4.attn.c_proj.bias\n",
            "h.4.ln_2.weight\n",
            "h.4.ln_2.bias\n",
            "h.4.mlp.c_fc.weight\n",
            "h.4.mlp.c_fc.bias\n",
            "h.4.mlp.c_proj.weight\n",
            "h.4.mlp.c_proj.bias\n",
            "h.5.ln_1.weight\n",
            "h.5.ln_1.bias\n",
            "h.5.attn.c_attn.weight\n",
            "h.5.attn.c_attn.bias\n",
            "h.5.attn.c_proj.weight\n",
            "h.5.attn.c_proj.bias\n",
            "h.5.ln_2.weight\n",
            "h.5.ln_2.bias\n",
            "h.5.mlp.c_fc.weight\n",
            "h.5.mlp.c_fc.bias\n",
            "h.5.mlp.c_proj.weight\n",
            "h.5.mlp.c_proj.bias\n",
            "h.6.ln_1.weight\n",
            "h.6.ln_1.bias\n",
            "h.6.attn.c_attn.weight\n",
            "h.6.attn.c_attn.bias\n",
            "h.6.attn.c_proj.weight\n",
            "h.6.attn.c_proj.bias\n",
            "h.6.ln_2.weight\n",
            "h.6.ln_2.bias\n",
            "h.6.mlp.c_fc.weight\n",
            "h.6.mlp.c_fc.bias\n",
            "h.6.mlp.c_proj.weight\n",
            "h.6.mlp.c_proj.bias\n",
            "h.7.ln_1.weight\n",
            "h.7.ln_1.bias\n",
            "h.7.attn.c_attn.weight\n",
            "h.7.attn.c_attn.bias\n",
            "h.7.attn.c_proj.weight\n",
            "h.7.attn.c_proj.bias\n",
            "h.7.ln_2.weight\n",
            "h.7.ln_2.bias\n",
            "h.7.mlp.c_fc.weight\n",
            "h.7.mlp.c_fc.bias\n",
            "h.7.mlp.c_proj.weight\n",
            "h.7.mlp.c_proj.bias\n",
            "h.8.ln_1.weight\n",
            "h.8.ln_1.bias\n",
            "h.8.attn.c_attn.weight\n",
            "h.8.attn.c_attn.bias\n",
            "h.8.attn.c_proj.weight\n",
            "h.8.attn.c_proj.bias\n",
            "h.8.ln_2.weight\n",
            "h.8.ln_2.bias\n",
            "h.8.mlp.c_fc.weight\n",
            "h.8.mlp.c_fc.bias\n",
            "h.8.mlp.c_proj.weight\n",
            "h.8.mlp.c_proj.bias\n",
            "h.9.ln_1.weight\n",
            "h.9.ln_1.bias\n",
            "h.9.attn.c_attn.weight\n",
            "h.9.attn.c_attn.bias\n",
            "h.9.attn.c_proj.weight\n",
            "h.9.attn.c_proj.bias\n",
            "h.9.ln_2.weight\n",
            "h.9.ln_2.bias\n",
            "h.9.mlp.c_fc.weight\n",
            "h.9.mlp.c_fc.bias\n",
            "h.9.mlp.c_proj.weight\n",
            "h.9.mlp.c_proj.bias\n",
            "h.10.ln_1.weight\n",
            "h.10.ln_1.bias\n",
            "h.10.attn.c_attn.weight\n",
            "h.10.attn.c_attn.bias\n",
            "h.10.attn.c_proj.weight\n",
            "h.10.attn.c_proj.bias\n",
            "h.10.ln_2.weight\n",
            "h.10.ln_2.bias\n",
            "h.10.mlp.c_fc.weight\n",
            "h.10.mlp.c_fc.bias\n",
            "h.10.mlp.c_proj.weight\n",
            "h.10.mlp.c_proj.bias\n",
            "h.11.ln_1.weight\n",
            "h.11.ln_1.bias\n",
            "h.11.attn.c_attn.weight\n",
            "h.11.attn.c_attn.bias\n",
            "h.11.attn.c_proj.weight\n",
            "h.11.attn.c_proj.bias\n",
            "h.11.ln_2.weight\n",
            "h.11.ln_2.bias\n",
            "h.11.mlp.c_fc.weight\n",
            "h.11.mlp.c_fc.bias\n",
            "h.11.mlp.c_proj.weight\n",
            "h.11.mlp.c_proj.bias\n",
            "ln_f.weight\n",
            "ln_f.bias\n",
            "total num of keys: 148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load HuggingFace Weights into Keras GPT-2 Model"
      ],
      "metadata": {
        "id": "aeMsr-s3IrRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_hf_weights_into_keras_model(gpt2_keras, hf_weights):\n",
        "  # embeddings\n",
        "  gpt2_keras.embedding.embeddings.assign(hf_weights[\"wte.weight\"])\n",
        "  gpt2_keras.pos_embedding.assign(hf_weights[\"wpe.weight\"])\n",
        "\n",
        "  # decoder layers\n",
        "  for i, decoder_layer in enumerate(gpt2_keras.dec_layers):\n",
        "    # LN\n",
        "    decoder_layer.layernorm1.gamma.assign(hf_weights[f\"h.{i}.ln_1.weight\"])\n",
        "    decoder_layer.layernorm1.beta.assign(hf_weights[f\"h.{i}.ln_1.bias\"])\n",
        "\n",
        "    decoder_layer.layernorm2.gamma.assign(hf_weights[f\"h.{i}.ln_2.weight\"])\n",
        "    decoder_layer.layernorm2.beta.assign(hf_weights[f\"h.{i}.ln_2.bias\"])\n",
        "\n",
        "    # MHA\n",
        "    decoder_layer.self_attention.W_qkv.assign(hf_weights[f\"h.{i}.attn.c_attn.weight\"])\n",
        "    decoder_layer.self_attention.b_qkv.assign(hf_weights[f\"h.{i}.attn.c_attn.bias\"])\n",
        "    decoder_layer.self_attention.W_o.assign(hf_weights[f\"h.{i}.attn.c_proj.weight\"])\n",
        "    decoder_layer.self_attention.b_o.assign(hf_weights[f\"h.{i}.attn.c_proj.bias\"])\n",
        "\n",
        "    # FFN\n",
        "    decoder_layer.feed_forward.ff1.kernel.assign(hf_weights[f\"h.{i}.mlp.c_fc.weight\"])\n",
        "    decoder_layer.feed_forward.ff1.bias.assign(hf_weights[f\"h.{i}.mlp.c_fc.bias\"])\n",
        "    decoder_layer.feed_forward.ff2.kernel.assign(hf_weights[f\"h.{i}.mlp.c_proj.weight\"])\n",
        "    decoder_layer.feed_forward.ff2.bias.assign(hf_weights[f\"h.{i}.mlp.c_proj.bias\"])\n",
        "\n",
        "  # final layer norm layer\n",
        "  gpt2_keras.final_ln.gamma.assign(hf_weights[\"ln_f.weight\"])\n",
        "  gpt2_keras.final_ln.beta.assign(hf_weights[\"ln_f.bias\"])"
      ],
      "metadata": {
        "id": "k1X2Pw__sNzR"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_hf_weights_into_keras_model(gpt2, numpy_state_dict)"
      ],
      "metadata": {
        "id": "Yv8OdtGv12ag"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Cosine Similarity between HF GPT2 and Keras GPT2 outputs"
      ],
      "metadata": {
        "id": "l-lQ-1Tk13jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "random_input_ids = np.random.randint(0, tokenizer.vocab_size, (1, 10))\n",
        "torch_input = torch.tensor(random_input_ids)\n",
        "print(\"Random input tokens (IDs):\", random_input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkgtBKpMOIeu",
        "outputId": "64b24c73-68dc-4fde-a1e7-4235ea2285ee"
      },
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random input tokens (IDs): [[ 6374  1678 33827 16198  9914 27890 22299 43585 43689 42557]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "hf_output = hf_model(torch_input).logits\n",
        "print(hf_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cq6DhlhOWn8",
        "outputId": "b8727903-9c51-4e54-d423-8fdd16ef0281"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_input = tf.convert_to_tensor(random_input_ids)\n",
        "keras_output = gpt2(tf_input).numpy()\n",
        "print(keras_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoJKLUJSOYFC",
        "outputId": "250bde6d-ca1e-477f-b6d6-d60161559b02"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10, 50257)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_output_np = hf_output.detach().cpu().numpy()\n",
        "\n",
        "cos_sim = dot(hf_output_np.flatten(), keras_output.flatten()) / (norm(hf_output_np.flatten()) * norm(keras_output.flatten()))\n",
        "print(f\"Cosine Similarity between HF GPT2 and Keras GPT2 outputs: {cos_sim}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_OHfxzMskSl",
        "outputId": "8fdc8bff-655b-411c-b1cd-08faaf4e4a29"
      },
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between HF GPT2 and Keras GPT2 outputs: 0.980139970779419\n"
          ]
        }
      ]
    }
  ]
}